{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analytics Ref/Def Coursework - MSc in Big Data Analytics\n",
    "#### July 2019\n",
    "\n",
    "###### Name: A.B.Ameerdeen\n",
    "###### IIT ID: 2018099\n",
    "###### RGU ID: 1812896"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (a)\n",
    "\n",
    "Following are is the set of functions intended to be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jul  6 12:01:38 2019\n",
    "\n",
    "@author: ammar\n",
    "\"\"\"\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Use sklearn's metrics function for evaluation of classifiers\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import the two classification algorithms we want to use for the task\n",
    "# Based on the features we extract, we altogether have 6 combinations of models to train               \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "\n",
    "#for question C\n",
    "# Extract features using the extractors we defined\n",
    "#from feature_extractors import bow_extractor, tfidf_extractor\n",
    "def bow_extractor(corpus, ngram_range=(1,1)):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "    \n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "#from feature_extractors import averaged_word_vectorizer\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "#from feature_extractors import tfidf_weighted_averaged_word_vectorizer\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, \n",
    "                               tfidf_vocabulary, model, num_features):\n",
    "                                   \n",
    "    docs_tfidfs = [(doc, doc_tfidf) \n",
    "                   for doc, doc_tfidf \n",
    "                   in zip(corpus, tfidf_vectors)]\n",
    "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,\n",
    "                                   model, num_features)\n",
    "                    for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def question_c(my_dataset):\n",
    "    train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(my_dataset, test_data_proportion=0.3)\n",
    "\n",
    "    bow_vectorizer, bow_train_features = bow_extractor(tokens_to_sentences(train_corpus.Tokens) )  \n",
    "    bow_test_features = bow_vectorizer.transform(tokens_to_sentences(test_corpus.Tokens) ) \n",
    "\n",
    "    mnb = MultinomialNB()\n",
    "\n",
    "    mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                               train_features=bow_train_features,\n",
    "                                               train_labels=train_labels,\n",
    "                                               test_features=bow_test_features,\n",
    "                                               test_labels=test_labels)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def word2vec(my_dataset):\n",
    "    train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(my_dataset, test_data_proportion=0.3)\n",
    "    \n",
    "    # First normalize both the training and test data using our previous funcionts                                                                        \n",
    "    #from normalization import normalize_corpus\n",
    "    \n",
    "    norm_train_corpus =  train_corpus ;#normalize_corpus(train_corpus)\n",
    "    norm_test_corpus = test_corpus ; #normalize_corpus(test_corpus)  \n",
    "    \n",
    "    ''.strip()\n",
    "    \n",
    "    # Extract features using the extractors we defined\n",
    "    #bow features\n",
    "    bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus.Phrase)  \n",
    "    bow_test_features = bow_vectorizer.transform(norm_test_corpus.Phrase) \n",
    "    \n",
    "    tokenized_train = norm_train_corpus.Tokens #[nltk.word_tokenize(text)                  for text in norm_train_corpus.Phrase]\n",
    "    tokenized_test = norm_test_corpus.Tokens #[nltk.word_tokenize(text) for text in norm_test_corpus]  \n",
    "    \n",
    "    full_model = gensim.models.Word2Vec(tokenized_train,\n",
    "                                   size=500,\n",
    "                                   window=100,\n",
    "                                   min_count=30,\n",
    "                                   sample=1e-3)       \n",
    "    mnb = MultinomialNB()\n",
    "    print(full_model.wv.vocab);\n",
    "    # Multinomial Naive Bayes with bag of words features\n",
    "    mnb_bow_predictions_fulldataset = train_predict_evaluate_model(classifier=mnb,\n",
    "                                               train_features=bow_train_features,\n",
    "                                               train_labels=train_labels,\n",
    "                                               test_features=bow_test_features,\n",
    "                                               test_labels=test_labels) \n",
    "\n",
    "def generate_bow(tokenList):\n",
    "    index = 0;\n",
    "    print(\"Word List for Document \\n{0} \\n\".format(tokenList));\n",
    "    bow = []\n",
    "    for tokens in tokenList:\n",
    "        #bag_vector = np.zeros(len(tokens))\n",
    "        \n",
    "        c = Counter(tokens);\n",
    "        #i = 0\n",
    "        #for w in tokens:\n",
    "               \n",
    "         #   for sentence in tokenList:\n",
    "             \n",
    "                #for word in sentence:\n",
    "                 #   if word == w:\n",
    "                  #      print(word)\n",
    "                   #     bag_vector[i] += 1\n",
    "                        #print(\"{0}\\n{1}\\n\".format(tokens,np.array(bag_vector)))\n",
    "            #i = i +1\n",
    "        bow.append(c)\n",
    "    return bow\n",
    "    \n",
    "\n",
    "\n",
    "def load_stop_words():\n",
    "    stop_data = pd.read_csv('stop_words.csv', encoding = 'utf-8') #,encoding = 'ISO-8859-1'\n",
    "    stopwords = [];\n",
    "    for index,row in stop_data.iterrows():\n",
    "        stopwords.append(row[0])\n",
    "    return stopwords;\n",
    "        \n",
    "#create a stemmer\n",
    "def load_stem_dictionary():\n",
    "    stem_data = pd.read_csv('sinhalese_stems.csv') #,encoding = 'ISO-8859-1'\n",
    "    stems = pd.DataFrame(columns = ['Stem', 'Word'])\n",
    "    dictionary = {};\n",
    "    for index, row in stem_data.iterrows():\n",
    "         s = re.split(r'\\t+', row[0]);\n",
    "         dictionary[s[0]]=s[1];\n",
    "    return dictionary;\n",
    "\n",
    "def word_feats(words, stopwords):\n",
    "    return dict([(' '.join(word.split()), True) for word in words if ' '.join(word.split()) not in stopwords])\n",
    "\n",
    "\n",
    "def non_stop_words(sentence_tokens):\n",
    "    non_stop_tokens = [];\n",
    "    for sentence in sentence_tokens:\n",
    "        tokens = []\n",
    "        token_dictionary = (word_feats(sentence,stopwords))\n",
    "        for i in token_dictionary.keys():\n",
    "            tokens.append(i)\n",
    "        non_stop_tokens.append(tokens)\n",
    "    return non_stop_tokens;\n",
    "\n",
    "def extract_tokens(sentences):\n",
    "    result = [];\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = [];\n",
    "        s = sentence.replace('[\\'','')\n",
    "        s = s.replace('\\']','')\n",
    "        s = s.replace('\\'','');\n",
    "        for word in s.split(','):\n",
    "            sentence_tokens.append(word)\n",
    "        result.append(sentence_tokens)\n",
    "    return result;\n",
    "\n",
    "#stemming\n",
    "def load_stem_df():\n",
    "    stem_data = pd.read_csv('sinhalese_stems.csv')\n",
    "    stems = pd.DataFrame(columns = ['Stem', 'Word'])\n",
    "    \n",
    "    word = [];\n",
    "    stem = [];\n",
    "    for index, row in stem_data.iterrows():\n",
    "         s = re.split(r'\\t+', row[0]);\n",
    "         stem.append(s[1])\n",
    "         word.append(s[0])\n",
    "    stems.Stem = stem;\n",
    "    stems.Word = word;\n",
    "    return stems;\n",
    "    \n",
    "def get_number_of_tokens(sentences):\n",
    "    length = 0;\n",
    "    for words in sentences:\n",
    "        unique_words = set(words)             # == set(['a', 'b', 'c'])\n",
    "        unique_word_count = len(unique_words) # == 3\n",
    "        length += unique_word_count\n",
    "    return length\n",
    "\n",
    "def get_max_sentence_length(sentences):\n",
    "    initial_max_length = 0;\n",
    "    for sentence in sentences:\n",
    "        if (len(set(sentence)) > initial_max_length):\n",
    "           initial_max_length= len(set(sentence))\n",
    "    return initial_max_length;\n",
    "\n",
    "\n",
    "\n",
    "#histograms\n",
    "def show_token_histogram():    \n",
    "    token_count = get_number_of_tokens(dataset.Tokens)\n",
    "    stopwords_count = get_number_of_tokens(dataset.NonStop)\n",
    "    stem_count = get_number_of_tokens(dataset.Stems)\n",
    "\n",
    "    objects = ('Original', 'After removing Stop words', 'After Stemming')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = [token_count, stopwords_count, stem_count]\n",
    "    \n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('Tokens')\n",
    "    plt.title('Token reduction')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def show_max_length_histogram():\n",
    "    token_sentence_length = get_max_sentence_length(dataset.Tokens)\n",
    "    stopwords_sentence_length = get_max_sentence_length(dataset.NonStop)\n",
    "    stems_sentence_length = get_max_sentence_length(dataset.Stems)\n",
    "    \n",
    "    objects = ('Original', 'After removing Stop words', 'After Stemming')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = [token_sentence_length, stopwords_sentence_length, stems_sentence_length]\n",
    "    \n",
    "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('Max Length')\n",
    "    plt.title('Max length reduction')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def get_stem(word):\n",
    "    try:\n",
    "        stem = stem_df[stem_df.Word == ' '.join(word.split())].Stem;\n",
    "        if(len(stem) > 0):\n",
    "            return stem.max()\n",
    "        else:\n",
    "            return word\n",
    "        return \n",
    "    except Exception:\n",
    "        return word\n",
    "\n",
    "\n",
    "def stem_all(sentences):\n",
    "    stem_sentences = [];\n",
    "    cnt = 0;\n",
    "    for sentence in sentences:\n",
    " #       print(sentence)\n",
    "        cnt += 1;\n",
    "        stems = []\n",
    "        for word in sentence:\n",
    "            i = get_stem(word)\n",
    "            stems.append(i);\n",
    "        stem_sentences.append(stems)\n",
    "    return stem_sentences;\n",
    "\n",
    "def get_columns(data):\n",
    "    for col in data.columns: \n",
    "        print(col) \n",
    "\n",
    "# Divide the data into training and testing sets\n",
    "def prepare_datasets(corpus, test_data_proportion=0.3):\n",
    "    train, test = train_test_split(corpus, test_size=0.33, random_state=42)\n",
    "    return train  , test, train.IsHateSpeech, test.IsHateSpeech\n",
    "\n",
    "# We also remove empty documents since they would just add noise\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels\n",
    "    \n",
    "\n",
    "\n",
    "# Define function to calculate the 4 common mertics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('Given below are metrics for the model : ')\n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        2))\n",
    "    print('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        2))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        2))\n",
    "\n",
    "\n",
    "# Master function to call the above defined functions to perform the classification,\n",
    "# predict the results and evaluate predictions against the test data\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    model = classifier.fit(train_features, train_labels)\n",
    "    #predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evaluate model prediction performance   \n",
    "    get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions   ,model\n",
    "\n",
    "\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "def clean_ascii_sentence(phrase):\n",
    "    phrase = str(phrase).lower()\n",
    "    phrase = phrase.replace('<.*?>', '') #remove tags\n",
    "    phrase = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",phrase).split())\n",
    "    return phrase\n",
    "\n",
    "def clean_unicode_sentence(phrase):\n",
    "    phrase = str(phrase).lower()\n",
    "    phrase = phrase.replace('<.*?>', '')\n",
    "    return phrase\n",
    "    \n",
    "def seperate_and_clean_data(data):\n",
    "    ascii_data = pd.DataFrame(columns = ['PhraseNo', 'Phrase' , 'IsHateSpeech' , \"Tokens\"])\n",
    "    unicode_data = pd.DataFrame(columns = ['PhraseNo', 'Phrase' , 'IsHateSpeech' , \"Tokens\"]);\n",
    "\n",
    "    ascii_Phrase = []    \n",
    "    ascii_PhraseNo = []\n",
    "    ascii_IsHateSpeech =  []\n",
    "    ascii_Tokens = [];\n",
    "    unicode_Phrase = []    \n",
    "    unicode_PhraseNo = []\n",
    "    unicode_IsHateSpeech =  []\n",
    "    unicode_Tokens = []\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        ascii_sentence = [];\n",
    "        unicode_sentence = [];\n",
    "        default_wt = nltk.word_tokenize\n",
    "        tokens = default_wt(row[1].Phrase)\n",
    "        for word in tokens:\n",
    "            if(is_ascii(word)):\n",
    "                    word = clean_ascii_sentence(word);\n",
    "                    if( word != ''):\n",
    "                        ascii_sentence.append(word)\n",
    "            elif(word != ''):\n",
    "                word = clean_unicode_sentence(word);\n",
    "                unicode_sentence.append(word)\n",
    "        if(len(ascii_sentence) > 0 ):\n",
    "            ascii_Tokens.append(ascii_sentence)\n",
    "            if(row[1].IsHateSpeech=='YES'):\n",
    "                ascii_IsHateSpeech.append(1)\n",
    "            else:\n",
    "                ascii_IsHateSpeech.append(0)\n",
    "            ascii_PhraseNo.append(row[1].PhraseNo)\n",
    "            ascii_Phrase.append(tokens)\n",
    "        if(len(unicode_sentence) > 0 ):\n",
    "            unicode_Tokens.append(unicode_sentence)\n",
    "            if(row[1].IsHateSpeech=='YES'):\n",
    "                unicode_IsHateSpeech.append(1)\n",
    "            else:\n",
    "                unicode_IsHateSpeech.append(0)\n",
    "            unicode_PhraseNo.append(row[1].PhraseNo)\n",
    "            unicode_Phrase.append(tokens)\n",
    "        \n",
    "    ascii_data[\"Phrase\"]=ascii_Phrase\n",
    "    ascii_data[\"PhraseNo\"]=ascii_PhraseNo\n",
    "    ascii_data[\"IsHateSpeech\"]=ascii_IsHateSpeech\n",
    "    ascii_data[\"Tokens\"]=ascii_Tokens\n",
    "    \n",
    "    unicode_data[\"Phrase\"]=unicode_Phrase\n",
    "    unicode_data[\"PhraseNo\"]=unicode_PhraseNo\n",
    "    unicode_data[\"IsHateSpeech\"]=unicode_IsHateSpeech\n",
    "    unicode_data[\"Tokens\"]=unicode_Tokens\n",
    "    \n",
    "    return ascii_data, unicode_data;\n",
    "\n",
    "\n",
    "def tokens_to_sentences(tokens):\n",
    "    sentences = []\n",
    "    for sent in tokens:\n",
    "        unique_words = set(sent)\n",
    "        sentences.append(\" \".join(unique_words))\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinhalese sentence percentage : 72.44\n",
      "Singlish sentence percentage : 44.2\n",
      "Mixed percentage : 11.599999999999994\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('labelled_data.csv') #,encoding = 'ISO-8859-1'\n",
    "ascii_dataset , unicode_dataset= seperate_and_clean_data(data)\n",
    "\n",
    "ascii_dataset.to_csv('ascii_dataset.csv');\n",
    "unicode_dataset.to_csv('unicode_dataset.csv')\n",
    "\n",
    "print('Sinhalese sentence percentage : '+ str(100*(len(unicode_dataset)/len(data))))\n",
    "print('Singlish sentence percentage : '+ str(100*(len(ascii_dataset)/len(data))))\n",
    "print('Mixed percentage : ' + str(100- ((100*(len(ascii_dataset)/len(data))) + (100*(len(ascii_dataset)/len(data)))) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is divided into singlish and sinhala datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (b)\n",
    "\n",
    "Text normalization happens below. We are basically removing stop words and stemming the corpus here. Then see how much we have cleaned the dataset using two diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ammar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\ammar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "stem_dictionary = load_stem_dictionary()\n",
    "stopwords = load_stop_words()\n",
    "stem_df = load_stem_df()\n",
    "\n",
    "dataset= unicode_dataset #pd.read_csv('sinhalese_data.csv') #,encoding = 'ISO-8859-1'\n",
    "\n",
    "#stop word removal, stemming\n",
    "#dataset.Tokens = extract_tokens(dataset.Tokens)\n",
    "dataset.NonStop = non_stop_words(dataset.Tokens)\n",
    "dataset.Stems = stem_all(dataset.NonStop)\n",
    "\n",
    "dataset.to_csv('non_stop_stem_sinhalese.csv')\n",
    "\n",
    "show_token_histogram()\n",
    "show_max_length_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (c)\n",
    "\n",
    "1) Corpus is split to training and test sets.\n",
    "\n",
    "2). Bag of words vectorizer is created using CountVectorizer. \n",
    "\n",
    "3). Then the same function returns us the training feature vector.\n",
    "\n",
    "4). Then we create a test feature vector.\n",
    "\n",
    "5). We are using Multinomial Naive Bayes model.\n",
    "\n",
    "6). Then we train the model using training vector and test the trained model using test feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given below are metrics for the model : \n",
      "Accuracy: 0.72\n",
      "Precision: 0.72\n",
      "Recall: 0.72\n",
      "F1 Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(dataset, test_data_proportion=0.3)\n",
    "\n",
    "bow_vectorizer, bow_train_features = bow_extractor(tokens_to_sentences(train_corpus.Tokens) )  \n",
    "bow_test_features = bow_vectorizer.transform(tokens_to_sentences(test_corpus.Tokens) ) \n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (d)\n",
    "\n",
    "Repeating the question c for given datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the full dataset : \n",
      "Given below are metrics for the model : \n",
      "Accuracy: 0.69\n",
      "Precision: 0.69\n",
      "Recall: 0.69\n",
      "F1 Score: 0.69\n",
      "\n",
      "for the singlish dataset :\n",
      "Given below are metrics for the model : \n",
      "Accuracy: 0.67\n",
      "Precision: 0.66\n",
      "Recall: 0.67\n",
      "F1 Score: 0.66\n",
      "\n",
      "for the sinhalese dataset : \n",
      "Given below are metrics for the model : \n",
      "Accuracy: 0.72\n",
      "Precision: 0.72\n",
      "Recall: 0.72\n",
      "F1 Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "full_dataset = pd.concat([unicode_dataset, ascii_dataset])\n",
    "print('for the full dataset : ')\n",
    "question_c(full_dataset) #fulll dataset \n",
    "print('\\nfor the singlish dataset :')\n",
    "question_c(ascii_dataset) # singlish dataset\n",
    "print('\\nfor the sinhalese dataset : ')\n",
    "question_c(unicode_dataset) #sinhalese dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see according the the accuracy of these results is that models trained with the sinhala dataset predicts the result better than any other. The singlish dataset is performing lower than the remaining. But that could be due to data cleaning issues too. Anyhow, the difference between full dataset models and sinhala models is not that big either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given below are metrics for the model : \n",
      "Accuracy: 0.72\n",
      "Precision: 0.73\n",
      "Recall: 0.72\n",
      "F1 Score: 0.71\n"
     ]
    }
   ],
   "source": [
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(dataset, test_data_proportion=0.2)\n",
    "\n",
    "# TFIDF features\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(tokens_to_sentences(train_corpus.Tokens))  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(tokens_to_sentences(test_corpus.Tokens))    \n",
    "\n",
    "model = gensim.models.Word2Vec(tokens_to_sentences(train_corpus.Tokens),\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)      \n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "mnb_tfidf_predictions , model = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf score based model is performing very similar to the Bag of Words based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given below are metrics for the model : \n",
      "Accuracy: 0.61\n",
      "Precision: 0.62\n",
      "Recall: 0.61\n",
      "F1 Score: 0.58\n"
     ]
    }
   ],
   "source": [
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(dataset, test_data_proportion=0.2)\n",
    "\n",
    "# TFIDF features\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(tokens_to_sentences(train_corpus.Tokens) , ngram_range=(2,2))  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(tokens_to_sentences(test_corpus.Tokens))    \n",
    "\n",
    "\n",
    "tokenized_train = train_corpus.Tokens #[nltk.word_tokenize(text)                  for text in norm_train_corpus.Phrase]\n",
    "tokenized_test = test_corpus.Tokens #[nltk.word_tokenize(text) for text in norm_test_corpus]  \n",
    "\n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=500,\n",
    "                               window=100,\n",
    "                               min_count=30,\n",
    "                               sample=1e-3)      \n",
    "\n",
    "mnb = MultinomialNB()\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "mnb_tfidf_predictions , model = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The idea of overfitting is a model being too familiar to the testing data. To the point it predicts the training labels\n",
    "tremendously well but failing sharp when it comes to the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

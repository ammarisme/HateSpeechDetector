{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions for the rest of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Jul 22 22:44:42 2019\n",
    "\n",
    "@author: ammar\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\ammar\\\\Desktop\\\\Text-Analytics-Msc\\\\dl_lab\")\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "def clean_ascii_sentence(phrase):\n",
    "    phrase = str(phrase).lower()\n",
    "    phrase = phrase.replace('<.*?>', '') #remove tags\n",
    "    phrase = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",phrase).split())\n",
    "    return phrase\n",
    "\n",
    "def clean_unicode_sentence(phrase):\n",
    "    phrase = str(phrase).lower()\n",
    "    phrase = phrase.replace('<.*?>', '')\n",
    "    return phrase\n",
    "    \n",
    "def seperate_and_clean_data(data):\n",
    "    ascii_data = pd.DataFrame(columns = ['PhraseNo', 'Phrase' , 'IsHateSpeech' , \"Tokens\"])\n",
    "    unicode_data = pd.DataFrame(columns = ['PhraseNo', 'Phrase' , 'IsHateSpeech' , \"Tokens\"]);\n",
    "\n",
    "    ascii_Phrase = []    \n",
    "    ascii_PhraseNo = []\n",
    "    ascii_IsHateSpeech =  []\n",
    "    ascii_Tokens = [];\n",
    "    unicode_Phrase = []    \n",
    "    unicode_PhraseNo = []\n",
    "    unicode_IsHateSpeech =  []\n",
    "    unicode_Tokens = []\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        ascii_sentence = [];\n",
    "        unicode_sentence = [];\n",
    "        default_wt = nltk.word_tokenize\n",
    "        tokens = default_wt(row[1].Phrase)\n",
    "        for word in tokens:\n",
    "            if(is_ascii(word)):\n",
    "                    word = clean_ascii_sentence(word);\n",
    "                    if( word != ''):\n",
    "                        ascii_sentence.append(word)\n",
    "            elif(word != ''):\n",
    "                word = clean_unicode_sentence(word);\n",
    "                unicode_sentence.append(word)\n",
    "        if(len(ascii_sentence) > 0 ):\n",
    "            ascii_Tokens.append(ascii_sentence)\n",
    "            if(row[1].IsHateSpeech=='YES'):\n",
    "                ascii_IsHateSpeech.append(1)\n",
    "            else:\n",
    "                ascii_IsHateSpeech.append(0)\n",
    "            ascii_PhraseNo.append(row[1].PhraseNo)\n",
    "            ascii_Phrase.append(tokens)\n",
    "        if(len(unicode_sentence) > 0 ):\n",
    "            unicode_Tokens.append(unicode_sentence)\n",
    "            if(row[1].IsHateSpeech=='YES'):\n",
    "                unicode_IsHateSpeech.append(1)\n",
    "            else:\n",
    "                unicode_IsHateSpeech.append(0)\n",
    "            unicode_PhraseNo.append(row[1].PhraseNo)\n",
    "            unicode_Phrase.append(tokens)\n",
    "        \n",
    "    ascii_data[\"Phrase\"]=ascii_Phrase\n",
    "    ascii_data[\"PhraseNo\"]=ascii_PhraseNo\n",
    "    ascii_data[\"IsHateSpeech\"]=ascii_IsHateSpeech\n",
    "    ascii_data[\"Tokens\"]=ascii_Tokens\n",
    "    \n",
    "    unicode_data[\"Phrase\"]=unicode_Phrase\n",
    "    unicode_data[\"PhraseNo\"]=unicode_PhraseNo\n",
    "    unicode_data[\"IsHateSpeech\"]=unicode_IsHateSpeech\n",
    "    unicode_data[\"Tokens\"]=unicode_Tokens\n",
    "    \n",
    "    return ascii_data, unicode_data;\n",
    "\n",
    "\n",
    "def tokens_to_sentences(tokens):\n",
    "    sentences = []\n",
    "    for sent in tokens:\n",
    "        unique_words = set(sent)\n",
    "        sentences.append(\" \".join(unique_words))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def load_stop_words():\n",
    "    stop_data = pd.read_csv('stop_words.csv', encoding = 'utf-8') #,encoding = 'ISO-8859-1'\n",
    "    stopwords = [];\n",
    "    for index,row in stop_data.iterrows():\n",
    "        stopwords.append(row[0])\n",
    "    return stopwords;\n",
    "        \n",
    "#create a stemmer\n",
    "def load_stem_dictionary():\n",
    "    stem_data = pd.read_csv('sinhalese_stems.csv') #,encoding = 'ISO-8859-1'\n",
    "    stems = pd.DataFrame(columns = ['Stem', 'Word'])\n",
    "    dictionary = {};\n",
    "    for index, row in stem_data.iterrows():\n",
    "         s = re.split(r'\\t+', row[0]);\n",
    "         dictionary[s[0]]=s[1];\n",
    "    return dictionary;\n",
    "\n",
    "def load_stem_df():\n",
    "    stem_data = pd.read_csv('sinhalese_stems.csv')\n",
    "    stems = pd.DataFrame(columns = ['Stem', 'Word'])\n",
    "    \n",
    "    word = [];\n",
    "    stem = [];\n",
    "    for index, row in stem_data.iterrows():\n",
    "         s = re.split(r'\\t+', row[0]);\n",
    "         stem.append(s[1])\n",
    "         word.append(s[0])\n",
    "    stems.Stem = stem;\n",
    "    stems.Word = word;\n",
    "    return stems;\n",
    "    \n",
    "\n",
    "def word_feats(words, stopwords):\n",
    "    return dict([(' '.join(word.split()), True) for word in words if ' '.join(word.split()) not in stopwords])\n",
    "\n",
    "\n",
    "def non_stop_words(sentence_tokens):\n",
    "    non_stop_tokens = [];\n",
    "    for sentence in sentence_tokens:\n",
    "        tokens = []\n",
    "        token_dictionary = (word_feats(sentence,stopwords))\n",
    "        for i in token_dictionary.keys():\n",
    "            tokens.append(i)\n",
    "        non_stop_tokens.append(tokens)\n",
    "    return non_stop_tokens;\n",
    "\n",
    "\n",
    "def get_stem(word):\n",
    "    try:\n",
    "        stem = stem_df[stem_df.Word == ' '.join(word.split())].Stem;\n",
    "        if(len(stem) > 0):\n",
    "            return stem.max()\n",
    "        else:\n",
    "            return word\n",
    "        return \n",
    "    except Exception:\n",
    "        return word\n",
    "\n",
    "\n",
    "def stem_all(sentences):\n",
    "    stem_sentences = [];\n",
    "    cnt = 0;\n",
    "    for sentence in sentences:\n",
    " #       print(sentence)\n",
    "        cnt += 1;\n",
    "        stems = []\n",
    "        for word in sentence:\n",
    "            i = get_stem(word)\n",
    "            stems.append(i);\n",
    "        stem_sentences.append(stems)\n",
    "    return stem_sentences;\n",
    "\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ammar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\ammar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('labelled_data.csv') #,encoding = 'ISO-8859-1'\n",
    "ascii_dataset , unicode_dataset= seperate_and_clean_data(data)\n",
    "\n",
    "stem_dictionary = load_stem_dictionary()\n",
    "stopwords = load_stop_words()\n",
    "stem_df = load_stem_df()\n",
    "\n",
    "#stop word removal, stemming\n",
    "#dataset.Tokens = extract_tokens(dataset.Tokens)\n",
    "unicode_dataset.NonStop = non_stop_words(unicode_dataset.Tokens)\n",
    "unicode_dataset.Stems = stem_all(unicode_dataset.NonStop)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns= [\"sentence\", \"label\"]);\n",
    "df['sentence'] = tokens_to_sentences(unicode_dataset.Stems);\n",
    "df['label'] = unicode_dataset.IsHateSpeech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e7da3299cc62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentences_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "sentences_train, sentences_test, y_train, y_test = train_test_split(df['sentence'],df['label'] , test_size=0.25, random_state = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectoring training and test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test = vectorizer.transform(sentences_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and training nueral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "input_dim  = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense( 10, input_dim=input_dim , activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\" , \n",
    "              optimizer = \"adam\",\n",
    "              metrics = ['accuracy']\n",
    "              )\n",
    "model.summary()\n",
    "history  = model.fit(X_train, y_train, \n",
    "                     epochs = 100,  \n",
    "                     verbose = False, \n",
    "                     validation_data= (X_test, y_test),\n",
    "                     batch_size = 10\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
